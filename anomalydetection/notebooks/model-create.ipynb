{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9607a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a48b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3\n",
    "M = 5\n",
    "WINDOW_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca150fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code here is copy and pasted from Stephen (@whistle_posse)\n",
    "https://twitter.com/whistle_posse/status/1488656595114663939?s=20&t=lB_T74PcwZmlJ1rrdu8tfQ\n",
    "from this notebook\n",
    "https://github.com/StephenHogg/SCS/blob/main/SCS/layer.py\n",
    "\"\"\"\n",
    "class AbsPool(nn.Module):\n",
    "    def __init__(self, pooling_module=None, *args, **kwargs):\n",
    "        super(AbsPool, self).__init__()\n",
    "        self.pooling_layer = pooling_module(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pos_pool = self.pooling_layer(x)\n",
    "        neg_pool = self.pooling_layer(-x)\n",
    "        abs_pool = torch.where(pos_pool >= neg_pool, pos_pool, -neg_pool)\n",
    "        return abs_pool\n",
    "\n",
    "\n",
    "MaxAbsPool1d = partial(AbsPool, nn.MaxPool1d)\n",
    "MaxAbsPool2d = partial(AbsPool, nn.MaxPool2d)\n",
    "MaxAbsPool3d = partial(AbsPool, nn.MaxPool3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f752fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based on  and copy/pasted heavily from code\n",
    "https://github.com/ZeWang95/scs_pytorch/blob/main/scs.py\n",
    "from Ze Wang\n",
    "https://twitter.com/ZeWang46564905/status/1488371679936057348?s=20&t=lB_T74PcwZmlJ1rrdu8tfQ\n",
    "\n",
    "and code\n",
    "https://github.com/oliver-batchelor/scs_cifar/blob/main/src/scs.py\n",
    "from Oliver Batchelor\n",
    "https://twitter.com/oliver_batch/status/1488695910875820037?s=20&t=QOnrCRpXpOuC0XHApi6Z7A\n",
    "\n",
    "and the TensorFlow implementation\n",
    "https://colab.research.google.com/drive/1Lo-P_lMbw3t2RTwpzy1p8h0uKjkCx-RB\n",
    "and blog post\n",
    "https://www.rpisoni.dev/posts/cossim-convolution/\n",
    "from Raphael Pisoni\n",
    "https://twitter.com/ml_4rtemi5\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SharpenedCosineSimilarity(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        eps=1e-12,\n",
    "    ):\n",
    "        super(SharpenedCosineSimilarity, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.eps = eps\n",
    "        self.padding = int(padding)\n",
    "\n",
    "        w = torch.empty(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        nn.init.xavier_uniform_(w)\n",
    "        self.w = nn.Parameter(\n",
    "            w.view(out_channels, in_channels, -1), requires_grad=True)\n",
    "\n",
    "        self.p_scale = 10\n",
    "        p_init = 2**.5 * self.p_scale\n",
    "        self.p = nn.Parameter(torch.empty(out_channels))\n",
    "        nn.init.constant_(self.p, p_init)\n",
    "\n",
    "        self.q_scale = 100\n",
    "        self.q = nn.Parameter(torch.empty(1))\n",
    "        nn.init.constant_(self.q, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshaping for compatibility with the einsum-based implementation\n",
    "        w = self.w.reshape(\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size,\n",
    "        )\n",
    "        w_norm = torch.linalg.vector_norm(\n",
    "            w,\n",
    "            dim=(1, 2, 3),\n",
    "            keepdim=True,\n",
    "        )\n",
    "\n",
    "        q_sqr = (self.q / self.q_scale) ** 2\n",
    "\n",
    "        # a small difference: we add eps outside of the norm\n",
    "        # instead of inside in order to reuse the performant\n",
    "        # code of torch.linalg.vector_norm\n",
    "        w_normed = w / ((w_norm + self.eps) + q_sqr)\n",
    "\n",
    "        x_norm_squared = F.avg_pool1d(\n",
    "            x ** 2,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            #divisor_override=1, # we actually want sum_pool\n",
    "        ).sum(dim=1, keepdim=True)\n",
    "\n",
    "        y_denorm = F.conv1d(\n",
    "            x,\n",
    "            w_normed,\n",
    "            bias=None,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "\n",
    "        y = y_denorm / ((x_norm_squared + self.eps).sqrt() + q_sqr)\n",
    "\n",
    "        sign = torch.sign(y)\n",
    "\n",
    "        y = torch.abs(y) + self.eps\n",
    "        p_sqr = (self.p / self.p_scale) ** 2\n",
    "        y = y.pow(p_sqr.reshape(1, -1, 1, 1))\n",
    "        return sign * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9163af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CIFAR10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c_/4t3xzkbs3c92gmghc5kz12c80000gn/T/ipykernel_2776/1519264858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m training_set = CIFAR10(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CIFAR10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CIFAR10' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 100\n",
    "max_lr = .01\n",
    "n_runs = 1000\n",
    "\n",
    "# Allow for a version to be provided at the command line, as in\n",
    "# $ python3 demo_fashion_mnist.py v15\n",
    "if len(sys.argv) > 1:\n",
    "    version = sys.argv[1]\n",
    "else:\n",
    "    version = \"test\"\n",
    "\n",
    "training_set = CIFAR10(\n",
    "    root=os.path.join('.', 'data', 'CIFAR10'),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "testing_set = CIFAR10(\n",
    "    root=os.path.join('.', 'data', 'CIFAR10'),\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    ) # num_workers=4)\n",
    "testing_loader = DataLoader(\n",
    "    testing_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    ) # num_workers=4)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, window_size=1024, scs_kernel_size=16, abs_kernel_size=8):\n",
    "        super().__init__()\n",
    "        self.scs1 = SharpenedCosineSimilarity(\n",
    "            in_channels=window_size, out_channels=window_size, kernel_size=scs_kernel_size, padding=0)\n",
    "        self.pool1 = MaxAbsPool1d(kernel_size=abs_kernel_size, stride=2, ceil_mode=True)\n",
    "        self.scs2 = SharpenedCosineSimilarity(\n",
    "            in_channels=window_size, out_channels=window_size*2, kernel_size=scs_kernel_size, padding=0)\n",
    "        self.pool2 = MaxAbsPool1d(kernel_size=abs_kernel_size, stride=2, ceil_mode=True)\n",
    "        self.scs3 = SharpenedCosineSimilarity(\n",
    "            in_channels=window_size*2, out_channels=window_size*4, kernel_size=scs_kernel_size, padding=0)\n",
    "        self.pool3 = MaxAbsPool1d(kernel_size=abs_kernel_size, stride=2, ceil_mode=True)\n",
    "        self.out = nn.Linear(in_features=window_size*4, out_features=window_size)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.scs1(t)\n",
    "        t = self.pool1(t)\n",
    "\n",
    "        t = self.scs2(t)\n",
    "        t = self.pool2(t)\n",
    "\n",
    "        t = self.scs3(t)\n",
    "        t = self.pool3(t)\n",
    "\n",
    "        t = t.reshape(-1, window_size*4)\n",
    "        t = self.out(t)\n",
    "\n",
    "        return torch.sigmoid(t)\n",
    "\n",
    "\n",
    "# Restore any previously generated results.\n",
    "try:\n",
    "    accuracy_results = np.load(accuracy_results_path).tolist()\n",
    "    accuracy_histories = np.load(accuracy_history_path).tolist()\n",
    "    loss_results = np.load(loss_results_path).tolist()\n",
    "except Exception:\n",
    "    loss_results = []\n",
    "    accuracy_results = []\n",
    "    accuracy_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de842910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    to inject anomalous points according to the formula in the paper:\n",
    "    \"\"\"\n",
    "    def __init__(self, win_siz, step, nums):\n",
    "        self.control = 0\n",
    "        self.win_siz = win_siz\n",
    "        self.step = step\n",
    "        self.number = nums\n",
    "\n",
    "    def generate_train_data(self, value, back_k=0, insert_anomaly=True):\n",
    "        def normalize(a):\n",
    "            amin = np.min(a)\n",
    "            amax = np.max(a)\n",
    "            a = (a - amin) / (amax - amin + 1e-5)\n",
    "            return 3 * a\n",
    "\n",
    "        if back_k <= 5:\n",
    "            back = back_k\n",
    "        else:\n",
    "            back = 5\n",
    "        length = len(value)\n",
    "        tmp = []\n",
    "        for pt in range(self.win_siz, length - back, self.step):\n",
    "            head = max(0, pt - self.win_siz)\n",
    "            tail = min(length - back, pt)\n",
    "            data = np.array(value[head:tail])\n",
    "            data = data.astype(np.float64)\n",
    "            data = normalize(data)\n",
    "            num = np.random.randint(1, self.number)\n",
    "            ids = np.random.choice(self.win_siz, num, replace=False)\n",
    "            lbs = np.zeros(self.win_siz, dtype=np.int64)\n",
    "            if insert_anomaly:\n",
    "                if (self.win_siz - 6) not in ids:\n",
    "                    self.control += np.random.random()\n",
    "                else:\n",
    "                    self.control = 0\n",
    "                if self.control > 100:\n",
    "                    ids[0] = self.win_siz - 6\n",
    "                    self.control = 0\n",
    "                mean = np.mean(data)\n",
    "                dataavg = average_filter(data)\n",
    "                var = np.var(data)\n",
    "                for id in ids:\n",
    "                    data[id] += (dataavg[id] + mean) * np.random.randn() * min((1 + var), 10)\n",
    "                    lbs[id] = 1\n",
    "            tmp.append([data.tolist(), lbs.tolist()])\n",
    "        return tmp\n",
    "\n",
    "    \n",
    "def average_filter(values, n=3):\n",
    "    \"\"\"\n",
    "    Calculate the sliding window average for the give time series.\n",
    "    Mathematically, res[i] = sum_{j=i-t+1}^{i} values[j] / t, where t = min(n, i+1)\n",
    "    :param values: list.\n",
    "        a list of float numbers\n",
    "    :param n: int, default 3.\n",
    "        window size.\n",
    "    :return res: list.\n",
    "        a list of value after the average_filter process.\n",
    "    \"\"\"\n",
    "\n",
    "    if n >= len(values):\n",
    "        n = len(values)\n",
    "\n",
    "    res = np.cumsum(values, dtype=float)\n",
    "    res[n:] = res[n:] - res[:-n]\n",
    "    res[n:] = res[n:] / n\n",
    "\n",
    "    for i in range(1, n):\n",
    "        res[i] /= (i + 1)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_next(values):\n",
    "    \"\"\"\n",
    "    Predicts the next value by sum up the slope of the last value with previous values.\n",
    "    Mathematically, g = 1/m * sum_{i=1}^{m} g(x_n, x_{n-i}), x_{n+1} = x_{n-m+1} + g * m,\n",
    "    where g(x_i,x_j) = (x_i - x_j) / (i - j)\n",
    "    :param values: list.\n",
    "        a list of float numbers.\n",
    "    :return : float.\n",
    "        the predicted next value.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(values) <= 1:\n",
    "        raise ValueError(f'data should contain at least 2 numbers')\n",
    "\n",
    "    v_last = values[-1]\n",
    "    n = len(values)\n",
    "\n",
    "    slopes = [(v_last - v) / (n - 1 - i) for i, v in enumerate(values[:-1])]\n",
    "\n",
    "    return values[1] + sum(slopes)\n",
    "\n",
    "\n",
    "def extend_series(values, extend_num=M, look_ahead=M):\n",
    "    \"\"\"\n",
    "    extend the array data by the predicted next value\n",
    "    :param values: list.\n",
    "        a list of float numbers.\n",
    "    :param extend_num: int, default 5.\n",
    "        number of values added to the back of data.\n",
    "    :param look_ahead: int, default 5.\n",
    "        number of previous values used in prediction.\n",
    "    :return: list.\n",
    "        The result array.\n",
    "    \"\"\"\n",
    "\n",
    "    if look_ahead < 1:\n",
    "        raise ValueError('look_ahead must be at least 1')\n",
    "\n",
    "    extension = [predict_next(values[-look_ahead - 2:-1])] * extend_num\n",
    "    return np.concatenate((values, extension), axis=0)\n",
    "\n",
    "def load_kpi(csv_path):\n",
    "    kpis = {}\n",
    "    anomalies = 0\n",
    "    with open(csv_path) as f:\n",
    "        input = csv.reader(f, delimiter=',')\n",
    "        cnt = 0\n",
    "        for row in input:\n",
    "            if cnt == 0:\n",
    "                cnt += 1\n",
    "                continue\n",
    "            kpi = kpis.get(str(row[3]), [[], [], []])\n",
    "            kpi[0].append(int(row[0]))  # timestamp\n",
    "            kpi[1].append(float(row[1]))  # value\n",
    "            kpi[2].append(int(row[2]))  # label\n",
    "            kpis[str(row[3])] = kpi\n",
    "            cnt += 1\n",
    "            if int(row[2]) == 1:\n",
    "                anomalies += 1\n",
    "        print(\"Training data loaded. Total length: {}, number of anomalies: {}\".format(cnt, anomalies))\n",
    "        f.close()\n",
    "    return kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc3550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded. Total length: 3004067, number of anomalies: 79554\n"
     ]
    }
   ],
   "source": [
    "kpis = load_kpi(os.getcwd() + '/../spectral-residual/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6167f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "generator = DataGenerator(WINDOW_SIZE, 1, 3)\n",
    "for kpi in kpis.values():\n",
    "    in_value = kpi[1]\n",
    "    train_data = generator.generate_train_data(in_value)\n",
    "    training_data += train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5021896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, window=1024):\n",
    "        self.window = window\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = nn.Conv1d(window, window, kernel_size=1, stride=1, padding=0)\n",
    "        self.layer2 = nn.Conv1d(window, 2 * window, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2 * window, 4 * window)\n",
    "        self.fc2 = nn.Linear(4 * window, window)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.window, 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa60b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dfd8847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/i506171/IdeaProjects/upscale-sre-aiops/anomalydetection/notebooks'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6792d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/Users/i506171/IdeaProjects/upscale-sre-aiops/anomalydetection/serving/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9624e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "js=json.loads({\"name\": \"startup.cfg\", \"modelCount\": 1, \"models\": {\\\n",
    "  \"testsrcnn\": {\\\n",
    "    \"1.0\": {\\\n",
    "        \"defaultVersion\": \"true\",\\\n",
    "        \"marName\": \"testsrcnn.mar\",\\\n",
    "        \"minWorkers\": 1,\\\n",
    "        \"maxWorkers\": 1,\\\n",
    "        \"batchSize\": 8,\\\n",
    "        \"maxBatchDelay\": 50,\\\n",
    "        \"responseTimeout\": 120\\\n",
    "    }\\\n",
    "  }\\\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18219c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
