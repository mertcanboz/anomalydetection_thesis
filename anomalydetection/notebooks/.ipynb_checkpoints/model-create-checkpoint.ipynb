{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0db235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859bf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3\n",
    "M = 5\n",
    "WINDOW_SIZE=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f3b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code here is copy and pasted from Stephen (@whistle_posse)\n",
    "https://twitter.com/whistle_posse/status/1488656595114663939?s=20&t=lB_T74PcwZmlJ1rrdu8tfQ\n",
    "from this notebook\n",
    "https://github.com/StephenHogg/SCS/blob/main/SCS/layer.py\n",
    "\"\"\"\n",
    "class AbsPool(nn.Module):\n",
    "    def __init__(self, pooling_module=None, *args, **kwargs):\n",
    "        super(AbsPool, self).__init__()\n",
    "        self.pooling_layer = pooling_module(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pos_pool = self.pooling_layer(x)\n",
    "        neg_pool = self.pooling_layer(-x)\n",
    "        abs_pool = torch.where(pos_pool >= neg_pool, pos_pool, -neg_pool)\n",
    "        return abs_pool\n",
    "\n",
    "\n",
    "MaxAbsPool1d = partial(AbsPool, nn.MaxPool1d)\n",
    "MaxAbsPool2d = partial(AbsPool, nn.MaxPool2d)\n",
    "MaxAbsPool3d = partial(AbsPool, nn.MaxPool3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32078152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Based on  and copy/pasted heavily from code\n",
    "https://github.com/ZeWang95/scs_pytorch/blob/main/scs.py\n",
    "from Ze Wang\n",
    "https://twitter.com/ZeWang46564905/status/1488371679936057348?s=20&t=lB_T74PcwZmlJ1rrdu8tfQ\n",
    "\n",
    "and code\n",
    "https://github.com/oliver-batchelor/scs_cifar/blob/main/src/scs.py\n",
    "from Oliver Batchelor\n",
    "https://twitter.com/oliver_batch/status/1488695910875820037?s=20&t=QOnrCRpXpOuC0XHApi6Z7A\n",
    "\n",
    "and the TensorFlow implementation\n",
    "https://colab.research.google.com/drive/1Lo-P_lMbw3t2RTwpzy1p8h0uKjkCx-RB\n",
    "and blog post\n",
    "https://www.rpisoni.dev/posts/cossim-convolution/\n",
    "from Raphael Pisoni\n",
    "https://twitter.com/ml_4rtemi5\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class SharpenedCosineSimilarity(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        kernel_size=1,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        eps=1e-12,\n",
    "    ):\n",
    "        super(SharpenedCosineSimilarity, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.eps = eps\n",
    "        self.padding = int(padding)\n",
    "\n",
    "        w = torch.empty(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        nn.init.xavier_uniform_(w)\n",
    "        self.w = nn.Parameter(\n",
    "            w.view(out_channels, in_channels, -1), requires_grad=True)\n",
    "\n",
    "        self.p_scale = 10\n",
    "        p_init = 2**.5 * self.p_scale\n",
    "        self.register_parameter(\"p\", nn.Parameter(torch.empty(out_channels)))\n",
    "        nn.init.constant_(self.p, p_init)\n",
    "\n",
    "        self.q_scale = 100\n",
    "        self.register_parameter(\"q\", nn.Parameter(torch.empty(1)))\n",
    "        nn.init.constant_(self.q, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = unfold2d(\n",
    "            x,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding)\n",
    "        n, c, h, w, _, _ = x.shape\n",
    "        x = x.reshape(n,c,h,w,-1)\n",
    "\n",
    "        # After unfolded and reshaped, dimensions of the images x are\n",
    "        # dim 0, n: batch size\n",
    "        # dim 1, c: number of input channels\n",
    "        # dim 2, h: number of rows in the image\n",
    "        # dim 3, w: number of columns in the image\n",
    "        # dim 4, l: kernel size, squared\n",
    "        #\n",
    "        # The dimensions of the weights w are\n",
    "        # dim 0, v: number of output channels\n",
    "        # dim 1, c: number of input channels\n",
    "        # dim 2, l: kernel size, squared\n",
    "\n",
    "        square_sum = torch.sum(torch.square(x), [1, 4], keepdim=True)\n",
    "        x_norm = torch.add(\n",
    "            torch.sqrt(square_sum + self.eps),\n",
    "            torch.square(self.q / self.q_scale))\n",
    "\n",
    "        square_sum = torch.sum(torch.square(self.w), [1, 2], keepdim=True)\n",
    "        w_norm = torch.add(\n",
    "            torch.sqrt(square_sum + self.eps),\n",
    "            torch.square(self.q / self.q_scale))\n",
    "\n",
    "        x = torch.einsum('nchwl,vcl->nvhw', x / x_norm, self.w / w_norm)\n",
    "        sign = torch.sign(x)\n",
    "\n",
    "        x = torch.abs(x) + self.eps\n",
    "        x = x.pow(torch.square(self.p / self.p_scale).view(1, -1, 1, 1))\n",
    "        return sign * x\n",
    "\n",
    "\n",
    "def unfold2d(x, kernel_size:int, stride:int, padding:int):\n",
    "    x = F.pad(x, [padding]*4)\n",
    "    bs, in_c, h, w = x.size()\n",
    "    ks = kernel_size\n",
    "    strided_x = x.as_strided(\n",
    "        (bs, in_c, (h - ks) // stride + 1, (w - ks) // stride + 1, ks, ks),\n",
    "        (in_c * h * w, h * w, stride * w, stride, w, 1))\n",
    "    return strided_x\n",
    "\n",
    "def unfold1d(x, kernel_size:int, stride:int, padding:int):\n",
    "    x = F.pad(x, [padding]*4)\n",
    "    \n",
    "    return x.unfold(1, kernel_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de23f7b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CIFAR10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c_/4t3xzkbs3c92gmghc5kz12c80000gn/T/ipykernel_78761/1519264858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m training_set = CIFAR10(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CIFAR10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CIFAR10' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1024\n",
    "n_epochs = 100\n",
    "max_lr = .01\n",
    "n_runs = 1000\n",
    "\n",
    "# Allow for a version to be provided at the command line, as in\n",
    "# $ python3 demo_fashion_mnist.py v15\n",
    "if len(sys.argv) > 1:\n",
    "    version = sys.argv[1]\n",
    "else:\n",
    "    version = \"test\"\n",
    "\n",
    "training_set = CIFAR10(\n",
    "    root=os.path.join('.', 'data', 'CIFAR10'),\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "testing_set = CIFAR10(\n",
    "    root=os.path.join('.', 'data', 'CIFAR10'),\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    ) # num_workers=4)\n",
    "testing_loader = DataLoader(\n",
    "    testing_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    ) # num_workers=4)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, window_size=1024, scs_kernel_size=16, abs_kernel_size=8):\n",
    "        super().__init__()\n",
    "        self.scs1 = SharpenedCosineSimilarity(\n",
    "            in_channels=window_size, out_channels=window_size, kernel_size=scs_kernel_size, padding=0)\n",
    "        self.pool1 = MaxAbsPool1d(kernel_size=abs_kernel_size, stride=2, ceil_mode=True)\n",
    "        self.scs2 = SharpenedCosineSimilarity(\n",
    "            in_channels=window_size, out_channels=window_size*2, kernel_size=scs_kernel_size, padding=0)\n",
    "        self.pool2 = MaxAbsPool1d(kernel_size=abs_kernel_size, stride=2, ceil_mode=True)\n",
    "        self.scs3 = SharpenedCosineSimilarity(\n",
    "            in_channels=window_size*2, out_channels=window_size*4, kernel_size=scs_kernel_size, padding=0)\n",
    "        self.pool3 = MaxAbsPool1d(kernel_size=abs_kernel_size, stride=2, ceil_mode=True)\n",
    "        self.out = nn.Linear(in_features=window_size*4, out_features=window_size)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.scs1(t)\n",
    "        t = self.pool1(t)\n",
    "\n",
    "        t = self.scs2(t)\n",
    "        t = self.pool2(t)\n",
    "\n",
    "        t = self.scs3(t)\n",
    "        t = self.pool3(t)\n",
    "\n",
    "        t = t.reshape(-1, window_size*4)\n",
    "        t = self.out(t)\n",
    "\n",
    "        return torch.sigmoid(t)\n",
    "\n",
    "\n",
    "# Restore any previously generated results.\n",
    "try:\n",
    "    accuracy_results = np.load(accuracy_results_path).tolist()\n",
    "    accuracy_histories = np.load(accuracy_history_path).tolist()\n",
    "    loss_results = np.load(loss_results_path).tolist()\n",
    "except Exception:\n",
    "    loss_results = []\n",
    "    accuracy_results = []\n",
    "    accuracy_histories = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e31c4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    to inject anomalous points according to the formula in the paper:\n",
    "    \"\"\"\n",
    "    def __init__(self, win_siz, step, nums):\n",
    "        self.control = 0\n",
    "        self.win_siz = win_siz\n",
    "        self.step = step\n",
    "        self.number = nums\n",
    "\n",
    "    def generate_train_data(self, value, back_k=0, insert_anomaly=True):\n",
    "        def normalize(a):\n",
    "            amin = np.min(a)\n",
    "            amax = np.max(a)\n",
    "            a = (a - amin) / (amax - amin + 1e-5)\n",
    "            return 3 * a\n",
    "\n",
    "        if back_k <= 5:\n",
    "            back = back_k\n",
    "        else:\n",
    "            back = 5\n",
    "        length = len(value)\n",
    "        tmp = []\n",
    "        for pt in range(self.win_siz, length - back, self.step):\n",
    "            head = max(0, pt - self.win_siz)\n",
    "            tail = min(length - back, pt)\n",
    "            data = np.array(value[head:tail])\n",
    "            data = data.astype(np.float64)\n",
    "            data = normalize(data)\n",
    "            num = np.random.randint(1, self.number)\n",
    "            ids = np.random.choice(self.win_siz, num, replace=False)\n",
    "            lbs = np.zeros(self.win_siz, dtype=np.int64)\n",
    "            if insert_anomaly:\n",
    "                if (self.win_siz - 6) not in ids:\n",
    "                    self.control += np.random.random()\n",
    "                else:\n",
    "                    self.control = 0\n",
    "                if self.control > 100:\n",
    "                    ids[0] = self.win_siz - 6\n",
    "                    self.control = 0\n",
    "                mean = np.mean(data)\n",
    "                dataavg = average_filter(data)\n",
    "                var = np.var(data)\n",
    "                for id in ids:\n",
    "                    data[id] += (dataavg[id] + mean) * np.random.randn() * min((1 + var), 10)\n",
    "                    lbs[id] = 1\n",
    "            tmp.append([data.tolist(), lbs.tolist()])\n",
    "        return tmp\n",
    "\n",
    "    \n",
    "def average_filter(values, n=3):\n",
    "    \"\"\"\n",
    "    Calculate the sliding window average for the give time series.\n",
    "    Mathematically, res[i] = sum_{j=i-t+1}^{i} values[j] / t, where t = min(n, i+1)\n",
    "    :param values: list.\n",
    "        a list of float numbers\n",
    "    :param n: int, default 3.\n",
    "        window size.\n",
    "    :return res: list.\n",
    "        a list of value after the average_filter process.\n",
    "    \"\"\"\n",
    "\n",
    "    if n >= len(values):\n",
    "        n = len(values)\n",
    "\n",
    "    res = np.cumsum(values, dtype=float)\n",
    "    res[n:] = res[n:] - res[:-n]\n",
    "    res[n:] = res[n:] / n\n",
    "\n",
    "    for i in range(1, n):\n",
    "        res[i] /= (i + 1)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def predict_next(values):\n",
    "    \"\"\"\n",
    "    Predicts the next value by sum up the slope of the last value with previous values.\n",
    "    Mathematically, g = 1/m * sum_{i=1}^{m} g(x_n, x_{n-i}), x_{n+1} = x_{n-m+1} + g * m,\n",
    "    where g(x_i,x_j) = (x_i - x_j) / (i - j)\n",
    "    :param values: list.\n",
    "        a list of float numbers.\n",
    "    :return : float.\n",
    "        the predicted next value.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(values) <= 1:\n",
    "        raise ValueError(f'data should contain at least 2 numbers')\n",
    "\n",
    "    v_last = values[-1]\n",
    "    n = len(values)\n",
    "\n",
    "    slopes = [(v_last - v) / (n - 1 - i) for i, v in enumerate(values[:-1])]\n",
    "\n",
    "    return values[1] + sum(slopes)\n",
    "\n",
    "\n",
    "def extend_series(values, extend_num=M, look_ahead=M):\n",
    "    \"\"\"\n",
    "    extend the array data by the predicted next value\n",
    "    :param values: list.\n",
    "        a list of float numbers.\n",
    "    :param extend_num: int, default 5.\n",
    "        number of values added to the back of data.\n",
    "    :param look_ahead: int, default 5.\n",
    "        number of previous values used in prediction.\n",
    "    :return: list.\n",
    "        The result array.\n",
    "    \"\"\"\n",
    "\n",
    "    if look_ahead < 1:\n",
    "        raise ValueError('look_ahead must be at least 1')\n",
    "\n",
    "    extension = [predict_next(values[-look_ahead - 2:-1])] * extend_num\n",
    "    return np.concatenate((values, extension), axis=0)\n",
    "\n",
    "def load_kpi(csv_path):\n",
    "    kpis = {}\n",
    "    anomalies = 0\n",
    "    with open(csv_path) as f:\n",
    "        input = csv.reader(f, delimiter=',')\n",
    "        cnt = 0\n",
    "        for row in input:\n",
    "            if cnt == 0:\n",
    "                cnt += 1\n",
    "                continue\n",
    "            kpi = kpis.get(str(row[3]), [[], [], []])\n",
    "            kpi[0].append(int(row[0]))  # timestamp\n",
    "            kpi[1].append(float(row[1]))  # value\n",
    "            kpi[2].append(int(row[2]))  # label\n",
    "            kpis[str(row[3])] = kpi\n",
    "            cnt += 1\n",
    "            if int(row[2]) == 1:\n",
    "                anomalies += 1\n",
    "        print(\"Training data loaded. Total length: {}, number of anomalies: {}\".format(cnt, anomalies))\n",
    "        f.close()\n",
    "    return kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "960c13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loaded. Total length: 3004067, number of anomalies: 79554\n"
     ]
    }
   ],
   "source": [
    "kpis = load_kpi(os.getcwd() + '/../spectral-residual/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "generator = DataGenerator(WINDOW_SIZE, 1, 3)\n",
    "for kpi in kpis.values():\n",
    "    in_value = kpi[1]\n",
    "    train_data = generator.generate_train_data(in_value)\n",
    "    training_data += train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bec7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, window=1024):\n",
    "        self.window = window\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = nn.Conv1d(window, window, kernel_size=1, stride=1, padding=0)\n",
    "        self.layer2 = nn.Conv1d(window, 2 * window, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2 * window, 4 * window)\n",
    "        self.fc2 = nn.Linear(4 * window, window)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.window, 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28dbe46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f42f56",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d984cf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/IdeaProjects/upscale-sre-aiops/anomalydetection/serving/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c_/4t3xzkbs3c92gmghc5kz12c80000gn/T/ipykernel_79227/2134114043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'~/IdeaProjects/upscale-sre-aiops/anomalydetection/serving/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/IdeaProjects/upscale-sre-aiops/anomalydetection/serving/'"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), '~/IdeaProjects/upscale-sre-aiops/anomalydetection/serving/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810e234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
